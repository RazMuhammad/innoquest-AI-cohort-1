{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4b6C5b5pS0sW",
        "outputId": "f856762c-1134-459a-faea-fda7e03a82d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/67.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.5/43.5 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.1/44.1 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m412.2/412.2 kB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m63.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.7/69.7 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m628.3/628.3 kB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m49.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.6/109.6 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.6/61.6 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.8/67.8 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m51.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m45.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m323.9/323.9 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m41.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m64.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.6/278.6 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m47.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.8/50.8 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.8/55.8 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.8/70.8 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m46.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m452.6/452.6 kB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.17.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 5.29.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -qU langchain-groq langchain langchain_core \"langchain-chroma>=0.1.2\" langchain_community sentence_transformers fastembed"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "import os\n",
        "os.environ[\"GROQ_API_KEY\"] = getpass.getpass(\"Enter Key: \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iOheI8AKTS5w",
        "outputId": "3f024e73-cbdd-4144-abb2-30bbbef309a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter Key: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_groq import ChatGroq\n",
        "model = ChatGroq(\n",
        "    model = \"llama3-8b-8192\",\n",
        "    temperature=0,\n",
        "    max_tokens=None,\n",
        "    timeout=None,\n",
        "    max_retries = 2\n",
        ")"
      ],
      "metadata": {
        "id": "D5geXVc_Tzy-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = model.invoke(\"Translate this senetcne into urdu: Its very cold today\")\n",
        "print(result)\n",
        "print(result.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dYrOAwuuU9QD",
        "outputId": "41c545b9-6226-4ae6-ae2d-b712fa365356"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content='Here\\'s the translation:\\n\\n\"Aaj bahut thandi hai\"\\n\\n(Note: \"Aaj\" means \"today\", \"bahut\" means \"very\", \"thandi\" means \"cold\")' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 40, 'prompt_tokens': 23, 'total_tokens': 63, 'completion_time': 0.033333333, 'prompt_time': 0.004510568, 'queue_time': 0.017563521000000002, 'total_time': 0.037843901}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_a97cfe35ae', 'finish_reason': 'stop', 'logprobs': None} id='run-b7f892b2-2c8d-4213-9cac-786c175d13fe-0' usage_metadata={'input_tokens': 23, 'output_tokens': 40, 'total_tokens': 63}\n",
            "Here's the translation:\n",
            "\n",
            "\"Aaj bahut thandi hai\"\n",
            "\n",
            "(Note: \"Aaj\" means \"today\", \"bahut\" means \"very\", \"thandi\" means \"cold\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage"
      ],
      "metadata": {
        "id": "VVNYF36yV57S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    SystemMessage(content=\"Solve this math porblem\"),\n",
        "    HumanMessage(content=\"What is 2 + 65?\")\n",
        "]\n",
        "\n",
        "result = model.invoke(messages)\n",
        "print(result)\n",
        "print(result.content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xuy0Zk80WDNS",
        "outputId": "a437798d-82df-47cf-a945-94dcb9358753"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content='The answer to 2 + 65 is 67.' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 13, 'prompt_tokens': 29, 'total_tokens': 42, 'completion_time': 0.010833333, 'prompt_time': 0.003879783, 'queue_time': 0.019077775999999998, 'total_time': 0.014713116}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_ea58750df3', 'finish_reason': 'stop', 'logprobs': None} id='run-911f042f-b978-4c10-a4dc-f3af6e5ebd4f-0' usage_metadata={'input_tokens': 29, 'output_tokens': 13, 'total_tokens': 42}\n",
            "The answer to 2 + 65 is 67.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    SystemMessage(content=\"Solve this math porblem\"),\n",
        "    HumanMessage(content=\"What is 2 + 65?\"),\n",
        "    AIMessage(content=\"The answer is 67\"),\n",
        "    HumanMessage(content=\"What is 245 / 65?\")\n",
        "]\n",
        "\n",
        "result = model.invoke(messages)\n",
        "print(result)\n",
        "print(result.content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42z59w7WXtIE",
        "outputId": "cb86d7b4-6020-4f92-9af5-fa80ca512480"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content='The answer is 3.77' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 8, 'prompt_tokens': 52, 'total_tokens': 60, 'completion_time': 0.006666667, 'prompt_time': 0.006630725, 'queue_time': 0.018783994, 'total_time': 0.013297392}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_6a6771ae9c', 'finish_reason': 'stop', 'logprobs': None} id='run-780bf58e-d3a4-40cc-9945-1a212bf8e503-0' usage_metadata={'input_tokens': 52, 'output_tokens': 8, 'total_tokens': 60}\n",
            "The answer is 3.77\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n"
      ],
      "metadata": {
        "id": "InoG286nZCYB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    (\"system\", \"You are a comedian who tells jokes\"),\n",
        "    (\"human\", \"Tell me a {count} jokes  about {topic}\"),\n",
        "]\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_messages(messages)\n",
        "prompt = prompt_template.invoke({\"topic\": \"engineers\", \"count\": 5})\n",
        "print(prompt)\n",
        "result = model.invoke(prompt)\n",
        "print(result.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6sOLPLpeZhzH",
        "outputId": "ce55c685-2ce4-4d3c-9fcb-4e7eeabb2fde"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "messages=[SystemMessage(content='You are a comedian who tells jokes', additional_kwargs={}, response_metadata={}), HumanMessage(content='Tell me a 5 jokes  about engineers', additional_kwargs={}, response_metadata={})]\n",
            "Here are five jokes about engineers:\n",
            "\n",
            "1. Why did the engineer cross the road?\n",
            "\n",
            "To get to the other side... of the problem, because they're always trying to find a solution!\n",
            "\n",
            "2. Why did the engineer quit his job?\n",
            "\n",
            "Because he didn't get arrays! (get a raise)\n",
            "\n",
            "3. What did the engineer say when his wife asked him to take out the trash?\n",
            "\n",
            "\"I'm on it, I'll just design a more efficient garbage disposal system and then I'll get to it.\"\n",
            "\n",
            "4. Why did the engineer go to the doctor?\n",
            "\n",
            "Because he was feeling a little \"dis-connected\"!\n",
            "\n",
            "5. Why did the engineer go to the party?\n",
            "\n",
            "Because he heard it was a \"circuit\" party and he wanted to \"charge\" up his social life!\n",
            "\n",
            "Hope you found these jokes \"en-light-ening\"!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema.output_parser import StrOutputParser\n"
      ],
      "metadata": {
        "id": "T3pcjinfcone"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    (\"system\", \"You are a comedian who tells jokes\"),\n",
        "    (\"human\", \"Tell me a {count} jokes  about {topic}\"),\n",
        "]\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_messages(messages)\n",
        "\n",
        "chain = prompt_template | model | StrOutputParser()\n",
        "chain.invoke({\"topic\": \"engineers\", \"count\": 5})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "dVyWGiJecw-B",
        "outputId": "4ebfe917-5a44-479b-fe87-501036a72266"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Here are five jokes about engineers:\\n\\n1. Why did the engineer cross the road?\\n\\nTo get to the other side... of the problem, because they\\'re always trying to find a solution!\\n\\n2. Why did the engineer quit his job?\\n\\nBecause he didn\\'t get arrays! (get a raise)\\n\\n3. What did the engineer say when his wife asked him to take out the trash?\\n\\n\"I\\'m on it, I\\'ll just design a more efficient garbage disposal system and then I\\'ll get to it.\"\\n\\n4. Why did the engineer go to the doctor?\\n\\nBecause he was feeling a little \"dis-connected\"!\\n\\n5. Why did the engineer go to the party?\\n\\nBecause he heard it was a \"circuit\" party and he wanted to \"charge\" up his social life!\\n\\nHope you found these jokes \"en-light-ening\"!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema.runnable import RunnableBranch, RunnableLambda\n",
        "\n",
        "positive_feedback_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are a helpful assistant.\"),\n",
        "        (\"human\",\n",
        "         \"Generate a thank you note for this positive feedback: {feedback}.\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "negative_feedback_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are a helpful assistant.\"),\n",
        "        (\"human\",\n",
        "         \"Generate a thank you note for this negative feedback: {feedback}.\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "neutral_feedback_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are a helpful assistant.\"),\n",
        "        (\n",
        "            \"human\",\n",
        "            \"Generate a request for more details for this neutral feedback: {feedback}.\",\n",
        "        ),\n",
        "    ]\n",
        ")\n",
        "escalate_feedback_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are a helpful assistant.\"),\n",
        "        (\n",
        "            \"human\",\n",
        "            \"Generate a message to escalate this feedback to a human agent: {feedback}.\",\n",
        "        ),\n",
        "    ]\n",
        ")\n",
        "\n",
        "classification_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are a helpful assistant.\"),\n",
        "        (\"human\",\n",
        "         \"Classify the sentiment of this feedback as positive, negative, neutral, or escalate: {feedback}.\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "branches = RunnableBranch(\n",
        "    (\n",
        "        lambda x: \"positive\" in x,\n",
        "        positive_feedback_template | model | StrOutputParser()\n",
        "    ),\n",
        "    (\n",
        "        lambda x: \"negative\" in x,\n",
        "        negative_feedback_template | model | StrOutputParser()\n",
        "    ),\n",
        "    (\n",
        "        lambda x: \"neutral\" in x,\n",
        "        neutral_feedback_template | model | StrOutputParser()\n",
        "    ),\n",
        "    escalate_feedback_template | model | StrOutputParser()\n",
        ")\n",
        "\n",
        "classification_chain = (\n",
        "    classification_template | model | StrOutputParser()\n",
        ")\n",
        "\n",
        "chain = classification_chain | branches\n",
        "\n",
        "review = \"The product is perfect. I loved it.\"\n",
        "result = chain.invoke({\"feedback\": review})\n",
        "\n",
        "print(result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GYZuECATeYRf",
        "outputId": "3ce0b18a-45a3-41d8-ab04-9b118a052ec3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here's a potential thank-you note:\n",
            "\n",
            "Dear [Name],\n",
            "\n",
            "We're thrilled to hear that you've had a perfect experience with our product! Your enthusiasm and satisfaction mean the world to us, and we're so grateful that you've loved using it. Your positive feedback is a huge motivator for our team, and we're honored to have been able to bring a smile to your face.\n",
            "\n",
            "Thank you again for taking the time to share your thoughts with us. We're committed to continuing to deliver high-quality products that exceed your expectations, and we're excited to see what the future holds for our partnership.\n",
            "\n",
            "Best regards,\n",
            "[Your Name/Company]\n"
          ]
        }
      ]
    }
  ]
}